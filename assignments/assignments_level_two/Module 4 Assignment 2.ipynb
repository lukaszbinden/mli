{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See http://www.anotsorandomwalk.com/backpropagation-example-with-numbers-step-by-step/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases to learn with their initial values\n",
    "w1 = 0.82\n",
    "w2 = 0.53\n",
    "w3 = 0.44\n",
    "w4 = 0.15\n",
    "w5 = 0.61\n",
    "w6 = 0.39\n",
    "w7 = 0.11\n",
    "w8 = 0.41\n",
    "w9 = 0.50\n",
    "w10 = 0.45\n",
    "w11 = 0.23\n",
    "w12 = 0.68\n",
    "w13 = 0.49\n",
    "w14 = 0.71\n",
    "w15 = 0.23\n",
    "w16 = 0.21\n",
    "w17 = 0.44\n",
    "w18 = 0.62\n",
    "b1 = 0.5\n",
    "b2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return np.divide(1, (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target values of the sample the network is trained on:\n",
    "x1 = 2\n",
    "x2 = 5\n",
    "x3 = 1\n",
    "x4 = 4\n",
    "t1 = 0.05\n",
    "t2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error function (= mean squared error MSE)\n",
    "def error(olist, tlist):\n",
    "    return 0.5 * (np.square(olist[0] - tlist[0]) + np.square(olist[1] - tlist[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement our simple MLP neural network with one hidden layer\n",
    "def forward_propagation(x1, x2, x3, x4) -> Tuple[float, float, float, float, float]:\n",
    "    zh1 = w1 * x1 + w4 * x2 + w7 * x3 + w10 * x4 + b1\n",
    "    zh2 = w2 * x1 + w5 * x2 + w8 * x3 + w11 * x4 + b1\n",
    "    zh3 = w3 * x1 + w6 * x2 + w9 * x3 + w12 * x4 + b1\n",
    "    \n",
    "    h1 = sigmoid(zh1)\n",
    "    h2 = sigmoid(zh2)\n",
    "    h3 = sigmoid(zh3)\n",
    "    \n",
    "    zo1 = w13 * h1 + w15 * h2 + w17 * h3 + b2\n",
    "    zo2 = w14 * h1 + w16 * h2 + w18 * h3 + b2\n",
    "    \n",
    "    o1 = sigmoid(zo1)\n",
    "    o2 = sigmoid(zo2)\n",
    "    \n",
    "    return h1, h2, h3, o1, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from 1 forward propagation:\n",
      "Values of hidden neurons h1-h3:\n",
      "\th1: 0.9918374288468401\th2: 0.9973748797433398\th3: 0.9985719267115775\n",
      "Predictions (output nodes o1-o2):\n",
      "\to1: 0.8395344467680094\to2: 0.8841947961679333\n",
      "Ground truth (target values t1-t2):\n",
      "\tt1: 0.05\tt2: 0.1\n",
      "Total error: 0.6191630604850664\n"
     ]
    }
   ],
   "source": [
    "# FORWARD PROPAGATION\n",
    "# make one iteratoin through the network and show the total error.\n",
    "h1, h2, h3, o1, o2 = forward_propagation(x1, x2, x3, x4)\n",
    "e_init = error([o1, o2], [t1, t2])\n",
    "print(\"Results from 1 forward propagation:\")\n",
    "print(\"Values of hidden neurons h1-h3:\")\n",
    "print(f\"\\th1: {h1}\\th2: {h2}\\th3: {h3}\")\n",
    "print(\"Predictions (output nodes o1-o2):\")\n",
    "print(f\"\\to1: {o1}\\to2: {o2}\")\n",
    "print(\"Ground truth (target values t1-t2):\")\n",
    "print(f\"\\tt1: {t1}\\tt2: {t2}\")\n",
    "print(f\"Total error: {e_init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROPAGATION\n",
    "learning_rates = [0.01, 0.05, 1.0]\n",
    "iterations = [1, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(iterations:int, alpha:float):\n",
    "    global w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, b1, b2\n",
    "    # 1. perform forward operation\n",
    "    # NB: this is equivalent to stochastic gradient descent as we do \n",
    "    # the backprop for each sample (here only one sample though), our sample is:\n",
    "    # input: x1, x2, x3, x4; target: t1, t2\n",
    "    for iter in range(iterations):\n",
    "        # do forward propagation\n",
    "        h1, h2, h3, o1, o2 = forward_propagation(x1, x2, x3, x4)\n",
    "        \n",
    "        # calculate error\n",
    "        e = error([o1, o2], [t1, t2])\n",
    "        \n",
    "        # perform backpropagation:\n",
    "        # 1) calculate all gradients\n",
    "        dEdw18 = (o2 - t2) * o2 * (1 - o2) * h3\n",
    "        dEdw17 = (o1 - t1) * o1 * (1 - o1) * h3\n",
    "        \n",
    "        dEdw16 = (o2 - t2) * o2 * (1 - o2) * h2\n",
    "        dEdw15 = (o1 - t1) * o1 * (1 - o1) * h2\n",
    "        \n",
    "        dEdw14 = (o2 - t2) * o2 * (1 - o2) * h1\n",
    "        dEdw13 = (o1 - t1) * o1 * (1 - o1) * h1\n",
    "        \n",
    "        dEdb2 = (o1 - t1) * o1 * (1 - o1) + (o2 - t2) * o2 * (1 - o2)\n",
    "        \n",
    "        dEdh1 = (o1 - t1) * o1 * (1 - o1) * w13 + (o2 - t2) * o2 * (1 - o2) * w14\n",
    "        dEdh2 = (o1 - t1) * o1 * (1 - o1) * w15 + (o2 - t2) * o2 * (1 - o2) * w16\n",
    "        dEdh3 = (o1 - t1) * o1 * (1 - o1) * w17 + (o2 - t2) * o2 * (1 - o2) * w18\n",
    "        \n",
    "        dEdw12 = dEdh3 * h3 * (1 - h3) * x4\n",
    "        dEdw11 = dEdh2 * h2 * (1 - h2) * x4\n",
    "        dEdw10 = dEdh1 * h1 * (1 - h1) * x4\n",
    "        \n",
    "        dEdw9 = dEdh3 * h3 * (1 - h3) * x3\n",
    "        dEdw8 = dEdh2 * h2 * (1 - h2) * x3\n",
    "        dEdw7 = dEdh1 * h1 * (1 - h1) * x3\n",
    "        \n",
    "        dEdw6 = dEdh3 * h3 * (1 - h3) * x2\n",
    "        dEdw5 = dEdh2 * h2 * (1 - h2) * x2\n",
    "        dEdw4 = dEdh1 * h1 * (1 - h1) * x2\n",
    "        \n",
    "        dEdw3 = dEdh3 * h3 * (1 - h3) * x1\n",
    "        dEdw2 = dEdh2 * h2 * (1 - h2) * x1\n",
    "        dEdw1 = dEdh1 * h1 * (1 - h1) * x1\n",
    "\n",
    "        dEdb1 = dEdh1 * h1 * (1 - h1) + dEdh2 * h2  * (1 - h2) + dEdh3 * h3 * (1 - h3)\n",
    "        \n",
    "        \n",
    "        # 2) update all weights and biases\n",
    "        # update weights w18 - w13 and b2\n",
    "        w18 = w18 - alpha * dEdw18\n",
    "        w17 = w17 - alpha * dEdw17\n",
    "        w16 = w16 - alpha * dEdw16\n",
    "        w15 = w15 - alpha * dEdw15\n",
    "        w14 = w14 - alpha * dEdw14\n",
    "        w13 = w13 - alpha * dEdw13\n",
    "        b2 = b2 - alpha * dEdb1\n",
    "        \n",
    "        # update weights w12 - w1 and b1\n",
    "        w12 = w12 - alpha * dEdw12\n",
    "        w11 = w11 - alpha * dEdw11\n",
    "        w10 = w10 - alpha * dEdw10\n",
    "        w9 = w9 - alpha * dEdw9\n",
    "        w8 = w8 - alpha * dEdw8\n",
    "        w7 = w7 - alpha * dEdw7\n",
    "        w6 = w6 - alpha * dEdw6\n",
    "        w5 = w5 - alpha * dEdw5\n",
    "        w4 = w4 - alpha * dEdw4\n",
    "        w3 = w3 - alpha * dEdw3\n",
    "        w2 = w2 - alpha * dEdw2\n",
    "        w1 = w1 - alpha * dEdw1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from 1 forward propagation after 1 backpropagation:\n",
      "Values of hidden neurons h1-h3:\n",
      "\th1: 0.9918044602061147\th2: 0.9973735762666279\th3: 0.9985710229371896\n",
      "Predictions (output nodes o1-o2):\n",
      "\to1: 0.8352069207845401\to2: 0.8817114034280252\n",
      "Ground truth (target values t1-t2):\n",
      "\tt1: 0.05\tt2: 0.1\n",
      "Total error: 0.61381 (vs. 0.61916 w/o bp)\n",
      "Network weights and biases:\n",
      "\tw1  = {w1}\n",
      "\tw2  = {w2}\n",
      "\tw3  = {w3\n",
      "\tw4  = {w4}\n",
      "\tw5  = {w5}\n",
      "\tw6  = {w6}\n",
      "\tw7  = {w7}\n",
      "\tw8  = {w8}\n",
      "\tw9  = {w9}\n",
      "\tw10 = {w10}\n",
      "\tw11 = {w11}\n",
      "\tw12 = {w12}\n",
      "\tw13 = {w13}\n",
      "\tw14 = {w14}\n",
      "\tw15 = {w15}\n",
      "\tw16 = {w16}\n",
      "\tw17 = {w17}\n",
      "\tw18 = {w18}\n",
      "\tb1  = {b1}\n",
      "\tb2  = {b2}\n"
     ]
    }
   ],
   "source": [
    "# perform one backpropagation through the network for a learning rate of 0.1\n",
    "backpropagation(iterations=1, alpha=0.1)\n",
    "h1, h2, h3, o1, o2 = forward_propagation(x1, x2, x3, x4)\n",
    "e = error([o1, o2], [t1, t2])\n",
    "print(\"Results from 1 forward propagation after 1 backpropagation:\")\n",
    "print(\"Values of hidden neurons h1-h3:\")\n",
    "print(f\"\\th1: {h1}\\th2: {h2}\\th3: {h3}\")\n",
    "print(\"Predictions (output nodes o1-o2):\")\n",
    "print(f\"\\to1: {o1}\\to2: {o2}\")\n",
    "print(\"Ground truth (target values t1-t2):\")\n",
    "print(f\"\\tt1: {t1}\\tt2: {t2}\")\n",
    "print(f\"Total error: {round(e, 5)} (vs. {round(e_init, 5)} w/o bp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights():\n",
    "    print(f\"\\tw1  = {w1}\")\n",
    "    print(f\"\\tw2  = {w2}\")\n",
    "    print(f\"\\tw3  = {w3}\")\n",
    "    print(f\"\\tw4  = {w4}\")\n",
    "    print(f\"\\tw5  = {w5}\")\n",
    "    print(f\"\\tw6  = {w6}\")\n",
    "    print(f\"\\tw7  = {w7}\")\n",
    "    print(f\"\\tw8  = {w8}\")\n",
    "    print(f\"\\tw9  = {w9}\")\n",
    "    print(f\"\\tw10 = {w10}\")\n",
    "    print(f\"\\tw11 = {w11}\")\n",
    "    print(f\"\\tw12 = {w12}\")\n",
    "    print(f\"\\tw13 = {w13}\")\n",
    "    print(f\"\\tw14 = {w14}\")\n",
    "    print(f\"\\tw15 = {w15}\")\n",
    "    print(f\"\\tw16 = {w16}\")\n",
    "    print(f\"\\tw17 = {w17}\")\n",
    "    print(f\"\\tw18 = {w18}\")\n",
    "    print(f\"\\tb1  = {b1}\")\n",
    "    print(f\"\\tb2  = {b2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network weights and biases after 1 backpropagation:\n",
      "\tw1  = 0.8198232996701793\n",
      "\tw2  = 0.5299783598485821\n",
      "\tw3  = 0.439972453580711\n",
      "\tw4  = 0.14955824917544838\n",
      "\tw5  = 0.6099458996214552\n",
      "\tw6  = 0.38993113395177753\n",
      "\tw7  = 0.10991164983508968\n",
      "\tw8  = 0.409989179924291\n",
      "\tw9  = 0.4999862267903555\n",
      "\tw10 = 0.4496465993403587\n",
      "\tw11 = 0.22995671969716414\n",
      "\tw12 = 0.6799449071614221\n",
      "\tw13 = 0.4794504990904563\n",
      "\tw14 = 0.7020358307812165\n",
      "\tw15 = 0.21939160098722876\n",
      "\tw16 = 0.20199136664354855\n",
      "\tw17 = 0.4293788688118612\n",
      "\tw18 = 0.6119817547007638\n",
      "\tb1  = 0.5\n",
      "\tb2  = 0.4998870565497362\n"
     ]
    }
   ],
   "source": [
    "print(\"Network weights and biases after 1 backpropagation:\")\n",
    "print_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
